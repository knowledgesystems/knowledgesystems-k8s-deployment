apiVersion: batch/v1
kind: CronJob
metadata:
  labels:
    app.kubernetes.io/instance: cbioportal
  name: cbioportal-public-mysqldump-weekly
spec:
  schedule: "59 23 * * 6"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: cbioportal-mysql-dump-weekly
          nodeSelector:
            workload: cbio-dev
          tolerations:
            - key: "workload"
              operator: "Equal"
              value: "cbio-dev"
              effect: "NoSchedule"
          containers:
            - envFrom:
                - secretRef:
                    name: cbioportal-mysql-dump
              env:
                - name: PUBLIC_BLUE_DB_NAME
                  valueFrom:
                    secretKeyRef:
                      name: cbioportal-public-blue
                      key: DB_PORTAL_DB_NAME
                - name: PUBLIC_GREEN_DB_NAME
                  valueFrom:
                    secretKeyRef:
                      name: cbioportal-public-green
                      key: DB_PORTAL_DB_NAME
                - name: PUBLIC_DB_HOST
                  valueFrom:
                    secretKeyRef:
                      name: cbioportal-public-blue
                      key: DB_HOST
              name: cbioportal-public-mysqldump-weekly
              image: ubuntu:22.04
              command: ["/bin/sh", "-c"]
              args:
                - printf "Install libraries\n";
                  apt-get update; apt-get install unzip curl less mysql-client -y;
                  printf "Done installing libraries\n\n";

                  printf "Install aws cli\n";
                  curl "https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip" -o "awscli.zip";
                  unzip awscli.zip;
                  ./aws/install;
                  printf "Done installing aws cli\n\n";

                  UPDATE_MGMT_DB_NAME="public_update_management_database";
                  GET_COLOR_SQL_QUERY="select current_database_in_production from update_status;";
                  GET_DB_SCHEMA_VERSION_SQL_QUERY="select DB_SCHEMA_VERSION from info;";
                  unset active_public_database_name;
                  active_database_color=$(mysql --host="${PUBLIC_DB_HOST}" --user="${DB_USER}" --password="${DB_PASSWORD}" "${UPDATE_MGMT_DB_NAME}" -s -N -e "${GET_COLOR_SQL_QUERY}");
                  if [ "${active_database_color}" = "blue" ];
                  then active_public_database_name="${PUBLIC_BLUE_DB_NAME}";
                  elif [ "${active_database_color}" = "green" ];
                  then active_public_database_name="${PUBLIC_GREEN_DB_NAME}";
                  else { printf "Unable to determine active database color/name\n"; exit 1; }
                  fi;
                  printf "active database name for backup is ${active_public_database_name}\n";
                  SCHEMA_VERSION=$(mysql --host="${PUBLIC_DB_HOST}" --user="${DB_USER}" --password="${DB_PASSWORD}" "${active_public_database_name}" -s -N -e "${GET_DB_SCHEMA_VERSION_SQL_QUERY}" | sed "s/\./_/g");
                  MYSQL_DUMP_FILE_NAME=dump_$(date "+%Y_%m_%d")_v${SCHEMA_VERSION}.sql.gz;

                  printf "Dump database to aws s3 bucket\n";
                  mysqldump --host="${PUBLIC_DB_HOST}" --user="${DB_USER}" --password="${DB_PASSWORD}" --set-gtid-purged=OFF --quick --compress "${active_public_database_name}" | gzip -c | aws s3 cp - "s3://${AWS_S3_DUMP_BUCKET}/dumps/${MYSQL_DUMP_FILE_NAME}";
                  printf "Done dumping to bucket\n\n";

                  printf "Delete overage dump files\n";
                  MAXIMUM_RETAINED_FILE_AGE_SECONDS=$((70*24*60*60));
                  bucket_file_list_file="$(mktemp)";
                  aws s3 ls "s3://${AWS_S3_DUMP_BUCKET}/dumps/" > "${bucket_file_list_file}";
                  while IFS='' read -r file_list_line; do
                  {
                  date_line_with_suffix="${file_list_line#*dump_}";
                  filename="dump_${date_line_with_suffix}";
                  date_line="${date_line_with_suffix%%_v*}";
                  year="${date_line%%_*}";
                  month_day="${date_line#*_}";
                  month="${month_day%%_*}";
                  day="${month_day#*_}";
                  seconds_since_epoc_now="$(date +%s)";
                  seconds_since_epoc_file="$(date +%s --date "${year}-${month}-${day}")";
                  age_of_file_in_seconds=$((${seconds_since_epoc_now}-${seconds_since_epoc_file}));
                  if [ "${age_of_file_in_seconds}" -gt "${MAXIMUM_RETAINED_FILE_AGE_SECONDS}" ] ; then
                  aws s3 rm "s3://${AWS_S3_DUMP_BUCKET}/dumps/${filename}";
                  fi;
                  }
                  done < "${bucket_file_list_file}";
                  rm "${bucket_file_list_file}";
                  printf "Done deleting overage dump files\n";

                  printf "Create an index page for object listing\n";
                  OBJECTS=$(aws s3 ls "s3://${AWS_S3_DUMP_BUCKET}/dumps/" | sort -r -k4 | awk '{size_gb = $3 / (1024^3); printf "<tr><td><a href=\"dumps/%s\">%s</a></td><td>%.2f GB</td><td><a href=\"dumps/%s\">Download</a></td></tr>\\n", $4, $4, size_gb, $4}');
                  aws s3api get-object --bucket ${AWS_S3_DUMP_BUCKET} --key site/template.html template.html > /dev/null;
                  sed "s|{{ S3_FILES }}|$OBJECTS|g" template.html > index.html;
                  aws s3 cp --content-type 'text/html' index.html "s3://${AWS_S3_DUMP_BUCKET}/index.html";
                  printf "Done creating index page\n";

          restartPolicy: OnFailure
