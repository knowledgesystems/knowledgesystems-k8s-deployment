[[{"l":"CDSI K8s Documentation","p":["Welcome to the documentation site for the Knowledge Systems Apps in CDSI at MSK. This site serves as the central resource for understanding, managing, and deploying the infrastructure and applications in the K8s repository."]},{"l":"Repo Structure","p":["account-number/→ Deployment files specific to each account","account-number/→ Terraform modules specific to each account","app-1.yaml→ Config file for App-1","app-1/→ Deployment files for App-1","app-2.yaml→ Config file for App-2","app-2/→ Deployment files for App-2","apps/","argocd/→ App-of-apps configuration for ArgoCD","argocd/→ GitOps configurations for Kubernetes applications","As part of this restructuring, the repository will transition to a clearer folder structure where all infrastructure-related code is in iac/, and all ArgoCD application configurations are in argocd/. This will improve maintainability and make it easier to onboard new contributors.","aws/→ Cloud provider","cluster-name/","clusters/","iac/→ Infrastructure as Code (Terraform, AWS, EKS)","main.tf","resource-type/→ Resource type, e.g. eks or s3 for AWS.","shared/→ Terraform modules for resources that live outside of a cluster.","terraform.tf","To eliminate the inconsistent directory structure in our repo, we are in the process of refactoring the directories. The goal is to group Infrastructure as Code (IaC) and ArgoCD-base GitOps configurations into dedicated folders:","variables.tf"]},{"l":"Contact","p":["If you are working on this repo and need help, reach out to us.","Github: knowledgesystems-k8s-deployment"]}],[{"l":"Infrastructure as Code (IaC)","p":["We are gradually adopting Terraform as our primary Infrastructure as Code (IAC) tool to provision and manage cloud resources in a scalable and consistent manner. While some infrastructure is still managed manually or through other tools, we are actively migrating towards a fully Terraform-driven approach. Our goal is to standardize infrastructure management by leveraging Git as the source of truth."]},{"l":"Terraform","p":["To improve resource tracking and management, we use Terraform to provision resources in AWS. The repo structure is setup to mirror this AWS setup and uses submodules to organize resources across multiple accounts. See Terraform for documentation on how to set up and manage Terraform."]},{"l":"Resource Tagging","p":["Proper tagging of all cloud resources provisioned through Terraform is required. This enables better resource tracking, accountability, cost analysis. There is a set of tags that should be attached to all existing and new resources. See Resource Tagging for further details."]}],[{"l":"Terraform","p":["The iac directory in the repo contains all Terraform configurations for managing the infrastructure across multiple AWS accounts and clusters."]},{"l":"Prerequisites","p":["terrafrom: Install the terraform CLI tool. The recommended way is to use tfenv to install and manage multiple versions of terraform as each cluster runs on a different version.","saml2aws: Before you can use terraform, you need to make sure you have setup saml2aws and you are logged in to the correct AWS account in your cli. For all of the following commands, terraform uses your awscli to connect to the aws account. Running these commands in the wrong account can lead to destructive actions."]},{"l":"AWS CLI Setup","p":["By default, submodules inside iac directory use the default aws cli profile when running commands. To use a custom profile, set the TF_VAR_AWS_PROFILE environment variable."]},{"l":"Usage","p":["The iac directory contains multiple submodules where each submodule resides under varying nesting levels. Essentially, any subdirectory that has *.tf files is a submodule. We don't manage a root module config to avoid accidental disastrous actions and changes that end up impacting multiple accounts at the same time. To make changes to each submodule, change your present working directory to that submodule where the *.tf files reside.","Change directory into the appropriate module. E.g., Do the following if you want to work on the eks setup under the my-cluster cluster in account 123456789abc.","Make sure your CLI is using the correct version of terraform that is mentioned in the terraform.tf file.","Now you can make changes to the Terraform files. After making changes format them to ensure consistent formatting.","Initialize terraform submodule. This creates intermediate lock and state files.","Pull remote module sources. This requires Git authentication when running for the first time.","Validate your terraform files.","Dry run to see what changes you are making.","Apply changes."]},{"l":"Creating New Submodules","p":["If you want to migrate from your current setup to Terraform-based resource management, follow the steps below to create a new submodule in the repo.","Create a submodule directory under the appropriate nesting structure.","Move into the new module.","Create a terraform.tf file for the terraform configuration for that specific submodule. Check other submodules in the repo for examples.","Make sure you set the required resource tags and state backend for every new submodule you create. See Resource Tagging and Infrastructure State."]},{"l":"Infrastructure State","p":["Making changes to state configuration, such as changing the bucket, key, or region, is a destructive action and can lead to out-of-sync terraform states. Always consult before making such changes as it would require state migration.","Terraform uses .tfstate files to keep track of the infrastructure state. By default, terraform stores these files locally within the submodule directory, which is insecure as state files can contain sensitive information. We use S3 Buckets to store our infrastructure state remotely. Each AWS account has a single S3 bucket called k8s-terraform-state-storage-account-number reserved for this purpose.","When creating a new submodule, create a new S3 bucket manually using AWS console and configure the submodule to use its own state file in the S3 bucket by adding a terraform.backend block to the terraform.tf file. See other submodules in the repo for examples.","When defining the backend block as shown below, make sure that the key is unique within that bucket."]}],[{"l":"Resource Tagging","p":["Tagging resources is very important for tracking our AWS costs. Proper tagging can be achieved through multiple ways, including manually adding tags through AWS console. For resources managed through Terraform, we use automated methods to inject tags into all resources."]},{"l":"Required Tags","p":["As part of our cost analysis, we have a set of required tags that need to be added to new and existing resources.","Tag Key","Details","Example","cdsi-app","Name of the app","cbioportal","cdsi-team","Team name within CDSI","data-visualization","cdsi-owner","Email of the resource owner","nasirz1@mskcc.org","For example, the following set of tags have been added to AWS resources related to cBioPortal."]},{"l":"Resource Scope","p":["There are a lot of AWS resources that we utilize. However, it is not possible nor practical to tag all those resources. After careful usage analysis, we decided that it's better to require tags for those resources that cost us the most. Therefore, when tagging new and existing resources, make sure the following services and resources are tagged at the very least.","AWS Service","Resource Type","EKS","Clusters, Nodegroups","EC2","Instances, Load balancers","RDS","Databases","S3","Buckets"]}],[{"l":"Kubernetes Deployment with GitOps","p":["We are gradually moving towards an Argo CD managed Kubernetes workflow. The goal is to have all manifests for managing applications across multiple AWS accounts and clusters within the argocd directory in the repo."]},{"l":"ArgoCD","p":["To eliminate syncing issues and ensure better management of our Kubernetes deployment, we use Argo CD. The K8s repo is used as the single source of truth for our deployments. We also follow the App-of-Apps pattern to structure our deployments, where a parent argocd application is responsible for managing all other apps within a cluster. The repo structure is setup to mirror follow this principle. See Argo CD for further details."]}],[{"l":"Argo CD","p":["The argocd directory in the repo contains all our manifests for managing the kubernetes deployments across multiple AWS accounts and clusters. We follow the app-of-apps workflow to manage everything. Each argocd/aws/account-number/clusters/cluster-name/apps directory has a argocd subdirectory that contains the parent app. This parent app is responsible for managing all other apps in ArgoCD within that cluster. See repo structure for details."]},{"l":"Syncing Changes","p":["The argocd directory contains manifests organized by aws-account/cluster-name/app-name. After making changes to manifests, push your changes to the repo. Once your changes are merged into the master branch, you can use the Argo CD dashboard to sync and deploy your changes."]},{"l":"Accessing the Argo CD Dashboard","p":["Argo CD dashboard can be accessed in multiple ways."]},{"l":"Public Instance","p":["For some of our clusters, we have attached the Argo CD dashboard to a public domain. See table below for the list of apps that have Argo CD instances publicly available.","App","Argo CD URL","cBioPortal","argocd.cbioportal.org"]},{"l":"Port-Forwarding","p":["By default, all ArgoCD installations come with a built-in dashboard that you can access by port-forwarding. You will need kubectl and access to the cluster for this method.","Confirm kubectl is using the correct context. E.g. if you want to work on the my-cluster cluster under account 123456789abc, your output should be:","Port-forward ArgoCD dashboard from the cluster to your localhost:8080.","Open ArgoCD at localhost:8080 and login with your credentials."]},{"l":"Creating New Apps","p":["Follow the steps below to add a new app to ArgoCD.","Create a new subdirectory under the appropriate nesting structure argocd/aws/account-number/clusters/cluster-name/apps/app-name. This subdirectory will host your deployment files.","In the argocd subdirectory under the same nesting level, add a new Application manifest. Look at other apps in the repo for examples.","Commit your changes. ArgoCD will automatically sync the changes and the new app should show up in the Dashboard."]},{"l":"Creating New Kubernetes Objects","p":["Follow the steps below to create new Kubernetes objects in a cluster (e.g. Deployment).","Add a new manifest file under the appropriate nesting structure argocd/aws/account-number/clusters/cluster-name/apps/app-name/object-name.yaml.","Push your changes to the repo.","Launch ArgoCD dashboard and refresh the app to fetch new changes from the repo.","Sync changes."]}],[{"l":"IP Blocking","p":["Due to certain regulations, we sometimes have to block access to our services for specific countries. This can be done by modifying the ingress helm values. The steps below shows how it was done for Genie instances of cBioPortal."]},{"l":"Country-based IP Blocking for Genie cBioPortal","p":["Append the following values to the values file. Add/remove countries to the http-snippet depending on your requirements:","Apply the new helm values. Make sure you set the maxMindLicenseKey with your license key:","Add http-snippet to the ingress rules that you want to block. For Genie portals, this was done by separating the ingress rules in its own file here"]}],[{"l":"Troubleshooting","p":["This list is used to track issues and their remedies."]},{"l":"MongoDB Persistent Volume Claim Error","p":["When installing MongoDB Helm Chart in a new cluster, we sometimes run into an error where the helm chart is unable to create a persistent volume, which leads to the persistent volume claim failing to bind:","This happens because in a new cluster, a default storage class for Kubernetes has not been set. Run the following commands to fix this:"]},{"l":"Nginx Ingress Helm Upgrade Errors","p":["For the 666628074417 account, upgrading Ingress controllers through Helm results in various errors due to the custom networking rules."]},{"l":"400 Bad Request - The plain HTTP request was sent to HTTPS port","p":["To fix this issue, apply the correct nginx controller config defined here after updating nginx."]},{"l":"Keycloak Invalid Requester Error","p":["Keycloak fails to forward headers, which leads to the following error in the network tab when the site is accessed:","The following error is seen in the kubernetes pods for the keycloak instance:","To solve this issue, use the correct forwarding rules by applying the configmap defined here after updating nginx."]},{"l":"Datadog failed to auto-detect cluster name","p":["When deployed on AWS EC2 nodes using the BOTTLEROCKET_* AMI types, Datadog fails to auto-detect the cluster name, resulting in the following errors:","To solve this, either use the AL2_* AMI type (NOT RECOMMENDED) or manually specify the cluster name in the Datadog helm values:"]},{"l":"Datadog always out-of-sync with helm and argocd","p":["By default, Datadog auto-assigns random values to the following on restarts:","Token used for authentication between the cluster agent and node agents.","Configmap for the APM Instrumentation KPIs.","This leads to Datadog being permanently out-of-sync when deployed with Helm and ArgoCD. To solve this issue, follow the steps below:","Provide a cluster agent token as a secret.","Generate a random 32-digit hexadecimal number.","Create a secret with the number generated above.","Use existing secret in Datadog helm values.","Disable API Instrumentation KPIs by setting the datadog.apm.instrumentation.skipKPITelemetry to false in the datadog helm values."]}],[{"l":"Incidents Log"},{"l":"2023/05/24 Mongodb session service crash","p":["Issue with session-service reported at 3.30PM","We identified that the mongo session service was down and had used up all disk space (20Gi) shortly. That same day we created a new session service with 100Gi storage but weren't able to recover all old sessions","On 2023/05/26 we were able to restore old sessions. We did lose two days of saved sessions (5/24-5/25)"]},{"l":"Remediation"},{"l":"Bring mongodb back","p":["Take snapshot of existing EBS volume using AWS (no auto snapshots were set up)","Set up new mongo database with helm: helm install cbioportal-session-service-mongo-20230524 --version 7.3.1 --set image.tag=4.2,persistence.size=100Gi bitnami/mongodb","Connect the session service to use that one (see commit)"]},{"l":"Bring session data back","p":["The mongo data was stored in an AWS snapshot in mongo's binary format, so not immediately accessible for re-import into another database. First we had to bring that back.","What didn't work:","Tried various approach of expanding existing k8s volume, but was tricky b/c volume expansion wasn't enabled for existing PersistenceVolumeClaims","What did work:","Instead, started a new AWS EC2 instance with that volume attached. For whatever reason i couldn’t see the attached volume within ubuntu at first (had to use lsblk and mount). Might be something you always have to do","Once the volume was accessible, we ran docker run bitnami/mongodb with the correct mount location specified to load the data","From a separate shell used mongodump as described (cmds are described in cbioportal/README)","Now that we got the dump, we set up a new mongo database in the k8s cluster to load the data: helm install cbioportal-session-service-mongo-4dot2-20230525 --set image.tag=4.2,persistence.size=100Gi bitnami/mongodb","Then we had to copy over the data into that k8s volume. Unf kubectl cp didn't work (some TCP timeout error). Instead we figured we could create a 2nd container int he mongodb pod with rsync to copy over the data into a container in the existing k8s deployment:"]}]]