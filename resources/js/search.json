[[{"l":"CDSI K8s Documentation","p":["Welcome to the documentation site for the Knowledge Systems Apps in CDSI at MSK. This site serves as the central resource for understanding, managing, and deploying the infrastructure and applications in the K8s repository."]},{"l":"Repo Structure","p":["account-number/→ Deployment files specific to each account","account-number/→ Terraform modules specific to each account","app-1.yaml→ Config file for App-1","app-1/→ Deployment files for App-1","app-2.yaml→ Config file for App-2","app-2/→ Deployment files for App-2","apps/","argocd/→ App-of-apps configuration for ArgoCD","argocd/→ GitOps configurations for Kubernetes applications","As part of this restructuring, the repository will transition to a clearer folder structure where all infrastructure-related code is in iac/, and all ArgoCD application configurations are in argocd/. This will improve maintainability and make it easier to onboard new contributors.","aws/→ Cloud provider","cluster-name/","clusters/","iac/→ Infrastructure as Code (Terraform, AWS, EKS)","main.tf","resource-type/→ Resource type, e.g. eks or s3 for AWS.","shared/→ Terraform modules shared by all accounts","terraform.tf","To eliminate the inconsistent directory structure in our repo, we are in the process of refactoring the directories. The goal is to group Infrastructure as Code (IaC) and ArgoCD-base GitOps configurations into dedicated folders:","variables.tf"]},{"l":"Contact","p":["If you are working on this repo and need help, reach out to us.","Github: knowledgesystems-k8s-deployment"]}],[{"i":"infrastructure-as-code-iac","l":"Infrastructure as Code (IaC)","p":["We are gradually adopting Terraform as our primary Infrastructure as Code (IAC) tool to provision and manage cloud resources in a scalable and consistent manner. While some infrastructure is still managed manually or through other tools, we are actively migrating towards a fully Terraform-driven approach. Our goal is to standardize infrastructure management by leveraging Git as the source of truth."]},{"l":"Terraform","p":["To improve resource tracking and management, we use Terraform to provision resources in AWS. The repo structure is setup to mirror this AWS setup and uses submodules to organize resources across multiple accounts. See Terraform for documentation on how to set up and manage Terraform."]},{"l":"Resource Tagging","p":["Proper tagging of all cloud resources provisioned through Terraform is required. This enables better resource tracking, accountability, cost analysis. There is a set of tags that should be attached to all existing and new resources. See Resource Tagging for further details."]},{"l":"Cost Analysis","p":["All the above efforts are eventually only useful if we closely monitor cloud costs. There are multiple ways to do this, including AWS Cost Explorer and CloudHealth. See [Cost Analysis] for further details."]},{"i":"iac-with-terraform-work-in-progress","l":"IAC with Terraform (Work in Progress)","p":["The iac directory contains all Terraform configurations for managing the infrastructure across multiple AWS accounts and clusters."]},{"l":"Prerequisites","p":["saml2aws: Before you can use terraform, you need to make sure you have setup saml2aws and you are logged in to the correct AWS account in your cli. For all of the following commands, terraform uses your awscli to connect to the aws account. Running these commands in the wrong account can lead to destructive actions."]},{"l":"AWS Cli Setup","p":["By default, submodules in this repo use the default aws cli profile when running commands. To use a custom profile, set the TF_VAR_AWS_PROFILE environment variable."]},{"l":"Usage","p":["The iac directory contains multiple submodules where each submodule resides under varying nesting levels. Essentially, any subdirectory that has *.tf files is a submodule. We don't manage a root module config to avoid accidental disastrous actions and changes that end up impacting multiple accounts at the same time. To make changes to each module, change your present working directory to that module where the *.tf files reside.","Change directory into the appropriate module. E.g., Do the following if you want to work on the EKS setup under the cbioportal-prod cluster in account 666628074417.","Now you can make changes to the Terraform files. After making changes format them to ensure consistent formatting.","Initialize terraform submodule. This creates intermediate lock and state files.","Pull remote module sources. This requires Git authentication when running for the first time.","Validate your terraform files.","Dry run to see what changes you are making.","Apply changes."]},{"l":"Creating New Submodule","p":["Follow the steps below to create a new submodule.","Create a submodule directory under the appropriate nesting structure.","Move into the new module.","Create a terraform.tf file for the terraform configuration for that specific submodule. Check other submodules in this repo for examples.","Make sure you set the required resource tags and state backend for every new submodule you create. See Resource Tagging and Infrastructure State."]},{"i":"resource-tagging-1","l":"Resource Tagging","p":["Tagging all submodule is very important for tracking our AWS costs. All PRs are checked by a linting Github Action to make sure proper tags have been added to each submodule. PRs are blocked in case of failure. When creating a new submodule or managing existing ones, make sure that the terraform.tf file contains the following tags within the provider aws block. Check existing submodules for examples."]},{"l":"Infrastructure State","p":["Making changes to state configuration, such as changing the bucket, key, or region, is a destructive action and can lead to out-of-sync terraform states. Always consult before making such changes as it would require state migration.","Terraform uses .tfstate files to keep track of the infrastructure state. By default, terraform stores these files locally within the module subdirectory, which is insecure as state files can contain sensitive information. We use S3 Buckets to store our infrastructure state remotely. Each AWS account has a single S3 bucket called k8s-terraform-state-storage-account-number reserved for this purpose.","When creating a new submodule, configure it to use its own state file in the S3 bucket by adding a backend block to the terraform.tf file. Check example."]}],[{"l":"Terraform","p":["The iac directory in the repo contains all Terraform configurations for managing the infrastructure across multiple AWS accounts and clusters."]},{"l":"Prerequisites","p":["saml2aws: Before you can use terraform, you need to make sure you have setup saml2aws and you are logged in to the correct AWS account in your cli. For all of the following commands, terraform uses your awscli to connect to the aws account. Running these commands in the wrong account can lead to destructive actions."]},{"l":"AWS CLI Setup","p":["By default, submodules inside iac directory use the default aws cli profile when running commands. To use a custom profile, set the TF_VAR_AWS_PROFILE environment variable."]},{"l":"Usage","p":["The iac directory contains multiple submodules where each submodule resides under varying nesting levels. Essentially, any subdirectory that has *.tf files is a submodule. We don't manage a root module config to avoid accidental disastrous actions and changes that end up impacting multiple accounts at the same time. To make changes to each submodule, change your present working directory to that submodule where the *.tf files reside.","Change directory into the appropriate module. E.g., Do the following if you want to work on the eks setup under the my-cluster cluster in account 123456789abc.","Now you can make changes to the Terraform files. After making changes format them to ensure consistent formatting.","Initialize terraform submodule. This creates intermediate lock and state files.","Pull remote module sources. This requires Git authentication when running for the first time.","Validate your terraform files.","Dry run to see what changes you are making.","Apply changes."]},{"l":"Creating New Submodules","p":["If you want to migrate from your current setup to Terraform-based resource management, follow the steps below to create a new submodule in the repo.","Create a submodule directory under the appropriate nesting structure.","Move into the new module.","Create a terraform.tf file for the terraform configuration for that specific submodule. Check other submodules in the repo for examples.","Make sure you set the required resource tags and state backend for every new submodule you create. See Resource Tagging and Infrastructure State."]},{"l":"Infrastructure State","p":["Making changes to state configuration, such as changing the bucket, key, or region, is a destructive action and can lead to out-of-sync terraform states. Always consult before making such changes as it would require state migration.","Terraform uses .tfstate files to keep track of the infrastructure state. By default, terraform stores these files locally within the submodule directory, which is insecure as state files can contain sensitive information. We use S3 Buckets to store our infrastructure state remotely. Each AWS account has a single S3 bucket called k8s-terraform-state-storage-account-number reserved for this purpose.","When creating a new submodule, create a new S3 bucket manually using AWS console and configure the submodule to use its own state file in the S3 bucket by adding a terraform.backend block to the terraform.tf file. See other submodules in the repo for examples."]}],[{"l":"Resource Tagging","p":["Tagging resources is very important for tracking our AWS costs. Proper tagging can be achieved through multiple ways, including manually adding tags through AWS console. For resources managed through Terraform, we use automated methods to inject tags into all resources."]},{"l":"Required Tags","p":["As part of our cost analysis, we have a set of required tags that need to be added to new and existing resources.","Tag Key","Details","Example","cdsi-app","Name of the app","cbioportal","cdsi-team","Team name within CDSI","data-visualization","cdsi-owner","Email of the resource owner","nasirz1@mskcc.org","For example, the following set of tags have been added to AWS resources related to cBioPortal."]},{"l":"Resource Scope","p":["There are a lot of AWS resources that we utilize. However, it is not possible nor practical to tag all those resources. After careful usage analysis, we decided that it's better to require tags for those resources that cost us the most. Therefore, when tagging new and existing resources, make sure the following services and resources are tagged at the very least.","AWS Service","Resource Type","EKS","Clusters, Nodegroups","EC2","Instances, Load balancers","RDS","Databases","S3","Buckets"]}],[{"l":"Kubernetes Deployment with GitOps","p":["We gradually moving towards an Argo CD managed Kubernetes workflow. The goal is to have all manifests for managing applications across multiple AWS accounts and clusters within the argocd directory in the repo."]},{"l":"ArgoCD","p":["To eliminate syncing issues and ensure better management of our Kubernetes deployment, we use Argo CD. The K8s repo is used as the single source of truth for our deployments. We also follow the App-of-Apps pattern to structure our deployments, where a parent argocd application is responsible for managing all other apps within a cluster. The repo structure is setup to mirror follow this principle. See Argo CD for further details."]}],[{"l":"Argo CD","p":["The argocd directory in the repo contains all our manifests for managing the kubernetes deployments across multiple AWS accounts and clusters. We follow the app-of-apps workflow to manage everything. Each argocd/aws/account-number/clusters/cluster-name/apps directory has a argocd subdirectory that contains the parent app. This parent app is responsible for managing all other apps in ArgoCD within that cluster. See repo structure for details."]},{"l":"Syncing Changes","p":["The argocd directory contains manifests organized by aws-account/cluster-name/app-name. After making changes to manifests, push your changes to the repo. Once your changes are merged into the master branch, you can use the Argo CD dashboard to sync and deploy your changes."]},{"l":"Accessing the Argo CD Dashboard","p":["Argo CD dashboard can be accessed in multiple ways."]},{"l":"Port-Forwarding","p":["By default, all ArgoCD installations come with a built-in dashboard that you can access by port-forwarding. You will need kubectl and access to the cluster for this method.","Confirm kubectl is using the correct context. E.g. if you want to work on the my-cluster cluster under account 123456789abc, your output should be:","Port-forward ArgoCD dashboard from the cluster to your localhost:8080.","Open ArgoCD at localhost:8080 and login with credentials. For credentials, email us."]},{"l":"Public Instance","p":["For some of our clusters, we have attached the Argo CD dashboard to a public domain. See table below for the list of apps that have Argo CD instance publicly available.","App","Argo CD URL","cBioPortal","argocd.cbioportal.org"]},{"l":"Creating New Apps","p":["Follow the steps below to add a new app to ArgoCD.","Create a new subdirectory under the appropriate nesting structure argocd/aws/account-number/clusters/cluster-name/apps/app-name. This subdirectory will host your deployment files.","In the argocd subdirectory under the same nesting level, add a new Application manifest. Look at other apps in the repo for examples.","Commit your changes. ArgoCD will automatically sync the changes and the new app should show up in the Dashboard."]},{"l":"Creating New Kubernetes Objects","p":["Follow the steps below to create new Kubernetes objects in a cluster (e.g. Deployment).","Add a new manifest file under the appropriate nesting structure argocd/aws/account-number/clusters/cluster-name/apps/app-name/object-name.yaml.","Push your changes to the repo.","Launch ArgoCD dashboard and refresh the app to fetch new changes from the repo.","Sync changes."]}],[{"l":"Incidents Log"},{"i":"20230524-mongodb-session-service-crash","l":"2023/05/24 Mongodb session service crash","p":["Issue with session-service reported at 3.30PM","We identified that the mongo session service was down and had used up all disk space (20Gi) shortly. That same day we created a new session service with 100Gi storage but weren't able to recover all old sessions","On 2023/05/26 we were able to restore old sessions. We did lose two days of saved sessions (5/24-5/25)"]},{"l":"Remediation"},{"l":"Bring mongodb back","p":["Take snapshot of existing EBS volume using AWS (no auto snapshots were set up)","Set up new mongo database with helm: helm install cbioportal-session-service-mongo-20230524 --version 7.3.1 --set image.tag=4.2,persistence.size=100Gi bitnami/mongodb","Connect the session service to use that one (see commit)"]},{"l":"Bring session data back","p":["The mongo data was stored in an AWS snapshot in mongo's binary format, so not immediately accessible for re-import into another database. First we had to bring that back.","What didn't work:","Tried various approach of expanding existing k8s volume, but was tricky b/c volume expansion wasn't enabled for existing PersistenceVolumeClaims","What did work:","Instead, started a new AWS EC2 instance with that volume attached. For whatever reason i couldn’t see the attached volume within ubuntu at first (had to use lsblk and mount). Might be something you always have to do","Once the volume was accessible, we ran docker run bitnami/mongodb with the correct mount location specified to load the data","From a separate shell used mongodump as described (cmds are described in cbioportal/README)","Now that we got the dump, we set up a new mongo database in the k8s cluster to load the data: helm install cbioportal-session-service-mongo-4dot2-20230525 --set image.tag=4.2,persistence.size=100Gi bitnami/mongodb","Then we had to copy over the data into that k8s volume. Unf kubectl cp didn't work (some TCP timeout error). Instead we figured we could create a 2nd container int he mongodb pod with rsync to copy over the data into a container in the existing k8s deployment:"]}]]