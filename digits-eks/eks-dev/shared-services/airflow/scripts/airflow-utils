#!/bin/bash

## Utility script for running Airflow commands
## Put in your .bashrc:
## export AIRFLOW_NS=<namespace>
## export AIRFLOW_DEPLOYMENT=</path/to/deployment/folder>
## export AIRFLOW_NODEGROUP=<nodegroup>
## source /path/to/airflow-utils

set -a # causes all functions/vars defined in this file to be exported to subshells

function afctl() {
    kubectl -n $AIRFLOW_NS "$@"
}

function scheduler-pod() {
    afctl get pod | cut -d' ' -f1 | grep ${AIRFLOW_NS}-scheduler | head -n1
}

function webserver-pod() {
    afctl get pod | cut -d' ' -f1 | grep ${AIRFLOW_NS}-webserver | head -n1
}

function worker-pods() {
    afctl get pod | cut -d' ' -f1 | grep -v scheduler | grep -v webserver
}

function scheduler-logs() {
    afctl logs $(scheduler-pod) -f -c scheduler
}

function delete-airflow-namespace() {
    kubectl delete namespace $AIRFLOW_NS
    kubectl create namespace $AIRFLOW_NS

    afctl apply -f $AIRFLOW_DEPLOYMENT/storage/pvc.yaml
    afctl apply -f $AIRFLOW_DEPLOYMENT/storage/pv.yaml

    afctl create cm airflow-cdm-setup --from-file=cdm-setup=$AIRFLOW_DEPLOYMENT/scripts/cdm-setup
    afctl create secret generic airflow-gcp-keyfile --from-file=gcp_key.json=$AIRFLOW_DEPLOYMENT/secrets/gcp_key.json
    afctl create secret generic airflow-git-secret --from-file=id_ed25519=$AIRFLOW_DEPLOYMENT/secrets/id_ed25519 --from-file=known_hosts=$AIRFLOW_DEPLOYMENT/secrets/known_hosts
    afctl create secret generic airflow-hpc3-ssh-keyfile --from-file=id_rsa=$AIRFLOW_DEPLOYMENT/secrets/id_rsa

    afctl apply -f $AIRFLOW_DEPLOYMENT/../ingress/${AIRFLOW_NS}-ingress.yaml
}

function delete-airflow-chart() {
    helm delete $AIRFLOW_NS --namespace $AIRFLOW_NS
}

function delete-airflow-pvcs() {
    # Delete all Airflow PVCs except for efs-pv-claim
    # note: this must be run before deleting the PVs
    pvcs_to_delete=(
        data-airflow-postgresql-0
        logs-airflow-triggerer-0
        logs-airflow-worker-0
        redis-db-airflow-redis-0
    )
    for pvc in "${pvcs_to_delete[@]}"; do
        # Force immediate deletion
        kubectl patch pvc $pvc -n $AIRFLOW_NS -p '{"metadata":{"finalizers":null}}'
        timeout 5 kubectl -n $AIRFLOW_NS delete pvc $pvc --grace-period=0 --force
    done

    afctl get pvc
}

function delete-airflow-pvs() {
    # Delete all Airflow PVs except for efs-pv
    pvs_to_delete=$(kubectl get pv | grep $AIRFLOW_NS | grep -v efs-pv | grep -v ${AIRFLOW_NS}-logs | cut -d' ' -f1)
    for pv in "${pvs_to_delete[@]}"; do
        # Force immediate deletion
        kubectl patch pv $pv -n $AIRFLOW_NS -p '{"metadata":{"finalizers":null}}'
        timeout 5 kubectl -n $AIRFLOW_NS delete pv $pv --grace-period=0 --force
    done

    afctl get pv
}

function delete-airflow-workers() {
    # Clean up worker pods
    worker_pods=$(worker-pods)
    for pod in "${worker_pods[@]}"; do
        timeout 5 kubectl -n $AIRFLOW_NS delete pod $pod
    done

    afctl get pod
}

function install-airflow-chart() {
    helm install -f $AIRFLOW_DEPLOYMENT/values.yaml -f $AIRFLOW_DEPLOYMENT/override-values.yaml $AIRFLOW_NS apache-airflow/airflow --namespace $AIRFLOW_NS --debug --timeout 10m
    #helm install -f $AIRFLOW_DEPLOYMENT/values.yaml -f $AIRFLOW_DEPLOYMENT/override-values.yaml $AIRFLOW_NS apache-airflow/airflow --namespace $AIRFLOW_NS --debug --timeout 10m --set nodeSelector."eks\.amazonaws\.com/nodegroup"=$AIRFLOW_NODEGROUP
}

function reboot-airflow-server() {
    # TODO: there is a volume leakage occurring every time this function is run--
    # the postgres backend for Airflow creates + attaches a new EBS volume each time, without
    # ever deleting the old one. This causes problems because a max of 26 volumes can be
    # attached to an EC2 Linux instance. We need to make sure that old volumes are deleted,
    # either by changing their lifecycle policy upon creation or deleting them manually in
    # this script.

    delete-airflow-namespace
    delete-airflow-chart
    delete-airflow-pvcs
    delete-airflow-pvs
    delete-airflow-workers
    install-airflow-chart
}

function ssh-into-pod() {
    pod=$1
    afctl exec -it $pod -- /bin/bash
}

function enter-image() {
    image=$1
    docker run --rm -it --entrypoint bash $image
}
